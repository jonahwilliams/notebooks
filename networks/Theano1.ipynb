{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Theano Tutorial - Preliminaries</h3>\n",
    "Here are just the basics of getting started with Theano for deep learning.  For starters, the common namespace imports are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Storing Datasets</h3>\n",
    "Ideally, we'd like to take advantage of GPU calculations to speed up our code.  Since most training algorithms use minibatches it might be tempting to define a new minibatch variable on every iteration - but this involves the expensive transfer of data from the CPU to GPU every cycle as well.  Instead it is common practice to have all minibatches in a shared variable and have the specific batch defined by slices.  Note that the correct datatype is always float - its a GPU!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from keras.utils import np_utils\n",
    "x = load_digits()['data']\n",
    "y = load_digits()['target']\n",
    "y = np_utils.to_categorical(y)\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shared_x = theano.shared(numpy.asarray(x, dtype=theano.config.floatX))\n",
    "shared_y = theano.shared(numpy.asarray(y, dtype=theano.config.floatX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore if we have a batch size of 100, accessing the third batch becomes the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = shared_x[2 * 100: 3 * 100]\n",
    "y_train = shared_y[2 * 100: 3 * 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loss Functions</h3>\n",
    "<h4>0-1 Loss</h4>\n",
    "\n",
    "$$l_{0, 1} = \\sum_{i=0}^{|D|}I_{f(x_i) \\neq y^{i}} $$\n",
    "\n",
    "can be implemented in theano as a symbolic function - it will be compiled into theano, not run in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "loss_zero_one = T.sum(T.neq(T.argmax(y_pred), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Negative Log-likelihood Loss</h4>\n",
    "A differentiable function, easier to optimize than the above.  Considers the probability of each predicted label versus the actual label.  Where $\\theta$ our the parameters of the model\n",
    "\n",
    "$$L(\\theta, D) = -\\sum_{i=0}^{|D|} log P(Y = y^{(i)} | x^{(i)}, \\theta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loss_nll = -T.sum(T.log(p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "# note on syntax: T.arange(y.shape[0]) is a vector of integers [0,1,2,...,len(y)].\n",
    "# Indexing a matrix M by the two vectors [0,1,...,K], [a,b,...,k] returns the\n",
    "# elements M[0,a], M[1,b], ..., M[K,k] as a vector.  Here, we use this\n",
    "# syntax to retrieve the log-probability of the correct labels, y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gradient Descent</h3>\n",
    "\n",
    "    while True:\n",
    "        loss = f(params)\n",
    "        d_loss = ... # compute gradient\n",
    "        params -= learning_rate * d_loss\n",
    "        if <stopping_condition_met>\n",
    "            return params\n",
    "        \n",
    "Calculating the gradient using every datapoint is very expensive, we can instead estimate it the gradient using only a single example\n",
    "\n",
    "<h3>Stochastic Gradient Descent</h3>\n",
    "\n",
    "    for (x_i,y_i) in training_set:\n",
    "        # imagine an infinite generator\n",
    "        # that may repeat examples (if there is only a finite training set)\n",
    "        loss = f(params, x_i, y_i)\n",
    "        d_loss_wrt_params = ... # compute gradient\n",
    "        params -= learning_rate * d_loss_wrt_params\n",
    "        if <stopping_condition_met>:\n",
    "            return params\n",
    "            \n",
    "Theano documentation recomends a twist on SGD using minibatches of fixed size.  This reduces variance somewhat while still speeding up calculations, though the advantage shrinks to nothing quickly with larger minibatch size.\n",
    "\n",
    "<h3>Minibatch Stochastic Gradient Descent</h3>\n",
    "\n",
    "    for (x_batch,y_batch) in train_batches:\n",
    "        # imagine an infinite generator\n",
    "        # that may repeat examples\n",
    "        loss = f(params, x_batch, y_batch)\n",
    "        d_loss_wrt_params = ... # compute gradient using theano\n",
    "        params -= learning_rate * d_loss_wrt_params\n",
    "        if <stopping_condition_met>:\n",
    "            return params\n",
    "\n",
    "Implementing minibatch SGD in theano looks like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " Minibatch Stochastic Gradient Descent\n",
    "\n",
    "# assume loss is a symbolic description of the loss function given\n",
    "# the symbolic variables params (shared variable), x_batch, y_batch;\n",
    "\n",
    "# compute gradient of loss with respect to params\n",
    "d_loss_wrt_params = T.grad(loss, params)\n",
    "\n",
    "# compile the MSGD step into a theano function\n",
    "updates = [(params, params - learning_rate * d_loss_wrt_params)]\n",
    "MSGD = theano.function([x_batch,y_batch], loss, updates=updates)\n",
    "\n",
    "for (x_batch, y_batch) in train_batches:\n",
    "    # here x_batch and y_batch are elements of train_batches and\n",
    "    # therefore numpy arrays; function MSGD also updates the params\n",
    "    print('Current loss is ', MSGD(x_batch, y_batch))\n",
    "    if stopping_condition_is_met:\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regularization</h3>\n",
    "shrinkage of the weight matrix has been shown to improve a model's ability to generalize.  This is done by adding an additional penalty term to the loss function.\n",
    "\n",
    "Where  $R(\\theta)$ is a function of the weights $\\theta$,\n",
    "\n",
    "$$ R(\\theta) = (\\sum_{i=1}^{|\\theta|} {|\\theta_i|}^{ \\ p})^{\\frac{1}{p}} $$\n",
    "\n",
    "Our new loss function becomes\n",
    "\n",
    "$$ NLL(\\theta, D) + \\lambda R(\\theta)$$\n",
    "\n",
    "<h4></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# symbolic Theano variable that represents the L1 regularization term\n",
    "L1  = T.sum(abs(param))\n",
    "\n",
    "# symbolic Theano variable that represents the squared L2 term\n",
    "L2_sqr = T.sum(param ** 2)\n",
    "\n",
    "# the loss\n",
    "loss = NLL + lambda_1 * L1 + lambda_2 * L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
