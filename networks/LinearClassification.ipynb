{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Linear Models for Classification</h3>\n",
    "For this notebook I will only consider the simpliest possible linear models, of the form\n",
    "\n",
    "$$ f(x_i, W, b) = Wx_i^T + b $$\n",
    "\n",
    "Where $x_i$ is a single data point $x_i = [x_i^1, x_i^2, ..., x_i^P]$, $W$ is a matrix of weights of shape $10 \\times P $ and b is a vector of shape $10 \\times 1$.\n",
    "\n",
    "First we'll load in our image data, then run some simple preprocessing on it so that all pixel values are between $-1$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = ['cifar-10-batches-py/data_batch_' + str(i) for i in range(1, 6)]\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "batches = map(unpickle, files)\n",
    "X = np.concatenate([b['data'] for b in batches])\n",
    "X = (X.astype(float) - 127) / 255\n",
    "y = np.concatenate([b['labels'] for b in batches])\n",
    "\n",
    "X_test = map(unpickle, ['cifar-10-batches-py/test_batch'])[0]['data']\n",
    "X_test = (X_test.astype(float) - 127) / 255\n",
    "y_test = map(unpickle, ['cifar-10-batches-py/test_batch'])[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12156863,  0.1254902 ,  0.14901961, ..., -0.01176471,\n",
       "         0.00784314, -0.06666667],\n",
       "       [ 0.42352941,  0.40784314,  0.41176471, ...,  0.2       ,\n",
       "         0.25098039,  0.28235294],\n",
       "       [ 0.12156863,  0.12156863,  0.04705882, ..., -0.46666667,\n",
       "        -0.48627451, -0.47058824],\n",
       "       ..., \n",
       "       [-0.41960784, -0.42352941, -0.43921569, ..., -0.30196078,\n",
       "        -0.29019608, -0.31372549],\n",
       "       [-0.4       , -0.43921569, -0.40784314, ..., -0.18431373,\n",
       "        -0.18039216, -0.18431373],\n",
       "       [-0.21176471, -0.11372549, -0.10980392, ..., -0.12941176,\n",
       "        -0.27058824, -0.39607843]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multiclass Support Vector Machine Loss</h3>\n",
    "In order to evaluate a linear model, we'll need a loss function to choose our parameters.  The Multiclass SVM loss is set up so that it \"wants\" to have the correct class score higher than the other scores by a certain margin.\n",
    "\n",
    "For the $i^{th}$ training example we are given a vector of data $x_i$ and label $y_i$.  The score function takes these and computes $f(x_i, y_i)$, the vector of class scores.  For each class in our class vector that is not the correct class $y_i$, we take the difference between the score for the correct class $y_i$ and for the current class $j$ plus a margin $\\Delta$.\n",
    "\n",
    "\n",
    "$$L_i = \\sum_{j \\neq y_i} max(0, f(x_i, W)_j - f(x_i, W)_{y_i} + \\Delta) $$\n",
    "\n",
    "For example, if $\\Delta = 10$ and there are three classes, the above might be\n",
    "\n",
    "\\begin{align}\n",
    "L_i &= max(0, 6 - 12 + 10) + max(0, 1 - 12 + 10)\\\\\n",
    "L_i &= 4\n",
    "\\end{align}\n",
    "\n",
    "Essentially what were are doing is checking if the different between the correct class and all other classes is at least $\\Delta$; beyond this point we do not care - the loss is always cutoff at 0.  The threshold at 0 is often called the Hinge Loss.  Sometimes people refer to the squared hinge loss (L2-SVM) which is $max(0, -)^2$\n",
    "\n",
    "<h3>Regularization</h3>\n",
    "The one problem with the loss function presented above is that the vector $W$ which correctly classifies all values may not be unique.  In order to express our preference for matricies with smaller weights, we will include regularization to shrink the weight sizes.  The most common would be $L2$ regularization - an elementwise penalty across all weights\n",
    "\n",
    "$$R(W) = \\sum_l \\sum_k w_{k, l}^2 $$\n",
    "\n",
    "The complete loss function\n",
    "\n",
    "$$L = \\frac{1}{N}\\sum_{i}L_i + \\lambda R(W) $$\n",
    "\n",
    "\n",
    "Now for a fully vectorized implementation in python (that is, no python for loops).  Where I have a dataset X of size $50000 \\times 3072$ and a weight vector of size $10 \\times 3072$ and a bias vector of size $10 \\times 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01161161, -0.00971034,  0.00239516, ..., -0.00631566,\n",
       "        -0.02677543, -0.00561924],\n",
       "       [-0.01272528, -0.01121538, -0.0129076 , ..., -0.0046118 ,\n",
       "         0.02102181,  0.00360619],\n",
       "       [-0.00800197, -0.0096109 ,  0.01541131, ...,  0.01569905,\n",
       "         0.00799403, -0.00427914],\n",
       "       ..., \n",
       "       [ 0.00979782, -0.00653428,  0.00300039, ..., -0.01664638,\n",
       "         0.00333352, -0.01273976],\n",
       "       [ 0.01605521,  0.00730659, -0.00376265, ..., -0.01462053,\n",
       "         0.0076531 , -0.01306435],\n",
       "       [ 0.00143793,  0.00062906,  0.00897753, ...,  0.00593007,\n",
       "        -0.00381795,  0.00241182]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = 0.01 * np.random.randn(10, 3072)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00742199],\n",
       "       [-0.01429153],\n",
       "       [ 0.01937989],\n",
       "       [-0.00217316],\n",
       "       [ 0.00604422],\n",
       "       [ 0.00149876],\n",
       "       [ 0.01263421],\n",
       "       [-0.01426743],\n",
       "       [ 0.00351166],\n",
       "       [ 0.00139518]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 0.01 * np.random.randn(10, 1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = np.dot(W, X.T) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we have sucessfully calculated the SVM scores with the given weight and bias vector using only linear algebra operations - now to implement the SVM loss.  Where y is the correct class, it is also the index of the score vector we wish to ignore.  We need to convert this array into a binary mask and then set all of the selected indexes of L to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04111017,  0.13602194,  0.02151653, ..., -0.12320148,\n",
       "        0.03972904, -0.03654969])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.T[[i for i in xrange(len(L[0]))], y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06576129, -0.06460448,  0.14917444, -0.07659875,  0.12367278,\n",
       "        0.04947665,  0.04111017,  0.07135378,  0.01407084, -0.03879697])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet it worked!  Now we just calcluate the SVM loss over all the examples with some margin $\\Delta$ and then set the above indexed losses to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Correct = L.T[[i for i in xrange(len(L[0]))], y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89.79199547,  89.47713091,  89.58325037, ...,  91.69389299,\n",
       "        90.04576641,  89.99991378])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = L.T - Correct[:, np.newaxis] + 10\n",
    "R[[i for i in xrange(len(R))], y] = 0\n",
    "np.sum(R, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the loss for a particular example as well as for the entire function - of course the randomly selected values for the weights don't work well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Softmax Classifier</h3>\n",
    "One of the limitations of the SVM classifier is that the class scores returned are difficult to interpret - for instance if you are classifying Dog, Cat, Bird and your class scores are 10, -2, 3.  While you understand that Dog is the selected class - it is difficult to compare the scores beyond this.\n",
    "\n",
    "The softmax classifier alleviates this isssue by returning confidence scores - pseudo probabilities $\\in [0, 1]$, replacing the hinge loss with the cross entropy\n",
    "\n",
    "$$L_i = log(\\frac{e^{f_{y_i}}}{\\sum_i e^{f_{y_i}}}) $$ or equivalently\n",
    "\n",
    "$$L_i = -f_{y_i} + \\sum_i log(e^{f_{y_i}}) $$\n",
    "\n",
    "The cross entropy between a true distribution $p$ and an estimated distribution $q$ can be measured by the cross entropy,\n",
    "\n",
    "$$H(p, q) = - \\sum_x p(x) logq(x) $$\n",
    "\n",
    "The softmax classifier is therefore minimizing the distance between the normalized class scores from the softmax function and the correct label with probability mass entirely on the correct score $[0,0,0,...,1,0]$\n",
    "\n",
    "\n",
    "<h3>Numerical Stability</h3>\n",
    "The intermediate terms $e^f_{y_i}$ can potentially be very large, resulting in numerical problems when dividing.  To correct for this (without influenceing the results) you can multiply the top and bottom of the softmax function by some constant $C$, usually set to $C = -max(f_{y_i})$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{e^f_{y_i}}{\\sum_j e^f_{y_i}} &= \\frac{C e^f_{y_i}}{C \\sum_j e^f_{y_i}} \\\\\n",
    "&=  \\frac{e^{f_{y_i} + log(C) }}{\\sum_j e^{f_{y_i} + log(C)}}\n",
    "\\end{align}\n",
    "\n",
    "The final result is unchanged but numerical stability is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from IPython.kernel.zmq import kernelapp as app\n",
      "/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,  nan])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = np.array([123, 456, 789])\n",
    "np.exp(f) / np.sum(np.exp(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.75274406e-290,   2.39848787e-145,   1.00000000e+000])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f -= np.max(f)\n",
    "np.exp(f) / np.sum(np.exp(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
