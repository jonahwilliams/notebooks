{
 "metadata": {
  "name": "",
  "signature": "sha256:4038bc6048ac4183d75ca526bfa38afe631e955fff4cf73a1be4987e779b7b57"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1>Overview of Supervised Learning</h1>\n",
      "Set of variables denoted as inputs, which are measured/preset.  These have some influence over one or more outputs.  For each example the goal is to predict the value of the outputs.\n",
      "\n",
      "qualitative data come from some finite set, such as the iris data.  predictions invvloving them are refered to as classifications.\n",
      "\n",
      "quantitative data is some measured value.  For historical reasons (Galton) predictions involving them are referd to as regressions.\n",
      "\n",
      "<h2>Least Squares and Nearest Neighbors</h2>\n",
      "The linear model method makes huge assumptions about structure but yields stable but possibly innacurate predictions.  The method of k-nearest neighbors makes very mild structural assumptions: its predictions are often more accurate but can be unstable.\n",
      "\n",
      "<h3>Linear Model and Least Squares</h3>\n",
      "Given a vector of inputs,\n",
      "\n",
      "$X^T = (X_1, X_2, ..., X_p)$\n",
      "\n",
      "We predict the output $Y$ via the model\n",
      "\n",
      "$ \\hat{Y} = \\hat{\\beta_0} + \\sum \\limits_{j=1}^p X_j\\hat{\\beta_j}$\n",
      "\n",
      "The term $\\beta_0$ is the intercept, also known as the Bias in machine learning.  Often it is convenient to include the constant variable 1 in X, include $\\beta_0$ in the vector of coefficients $\\hat{\\beta}$, and write the linear model in vector form\n",
      "\n",
      "$\\hat{Y} = X^T\\hat{B}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Viewed as a function across a p-dimensional input space, $f(X) =X^T\\beta$ is linear, and the gradient $f'(X)=\\beta$ is a vector in input space that points in the steepest uphill direction. \n",
      "\n",
      "The most common way to find a linear model is the method of leas squares.  We pick the coefficients $\\beta$ to minimize the residual sum of squares,\n",
      "\n",
      "$RSS(\\beta) = \\sum \\limits_{i=1}^N (y_i - x_i^T\\beta)^2$\n",
      "\n",
      "$RSS(\\beta)$ is a quadradic function of the parameters, and hence its maximum always exists, but may not be unique (multicolinearity).  THe solution is easiest to characterize in matrix notation.\n",
      "\n",
      "$RSS(\\beta) = (y - X\\beta)^T(y - X\\beta)$\n",
      "\n",
      "Where X is an N x p matrix with each row an input vector and y is an N-vector of the outputs in the training set.  Differentiating w.r.t. $\\beta$ we get the normal equations,\n",
      "\n",
      "$X^T(y - X\\beta) = 0$\n",
      "\n",
      "if $X^TX$ is nonsingular, then the unique solution is given by\n",
      "\n",
      "$\\hat{\\beta} = (X^TX)^{-1}X^Ty$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}