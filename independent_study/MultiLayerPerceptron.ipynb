{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MultiLayer Perceptron<br />\n",
    "<small>Using Theano</small></h2>\n",
    "\n",
    "An artificial neural network with one or more hidden layers is historically refered to as multilayer perceptron.  With a single hidden layer, let $X$ be the data , $h(X)$ be a nonlinear function, and $W_i$ and $b_i$ be the weights and biases of the $i^{th}$ layer.  Then we can write the activation function as\n",
    "\n",
    "$$f(h(X)) = f(W_2 \\cdot h(W_1 \\cdot X + b_1)  + b_2) $$\n",
    "\n",
    "\n",
    "First, we will create a generic Layer object to cimplify constructing our model.  Each will need to have a weight tensor and bias tensor, input and output parameters, which in turn will require input and output size and an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T \n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, input, n_in, n_out, activation):\n",
    "        \n",
    "        # Stores input tensor\n",
    "        self.input = input\n",
    "        # Randomly initialize weights to small values\n",
    "        W = np.asarray(rng.uniform(low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                                   high=np.sqrt(6. / (n_in + n_out)),\n",
    "                                   size=(n_in, n_out)),\n",
    "                       dtype=theano.config.floatX)\n",
    "        # Bias should be 0 vector\n",
    "        b = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        \n",
    "        self.W = theano.shared(value=W, name='W', borrow=True)\n",
    "        self.b = theano.shared(value=b, name='b', borrow=True)\n",
    "        \n",
    "        # Compute activation function\n",
    "        linear_output = T.dot(input, self.W) + self.b\n",
    "        if activation is None:\n",
    "            self.output = linear_output\n",
    "        else:\n",
    "            self.output = activation(linear_output)\n",
    "\n",
    "        # Collect parameters of the model\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a layer object, linking them together is as simple as creating them with the correct dimensions and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = T.lscalar() # Index for batches\n",
    "x = T.dmatrix('x')  # holds our datapoints\n",
    "y = T.ivector('y')  # holds the class labels\n",
    "\n",
    "hidden = Layer(input=x, n_in=4, n_out=16, activation=T.tanh)\n",
    "softmax = Layer(input=hidden.output, n_in=16, n_out=3, activation=T.nnet.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need a complete list of our model parameters, a cost function, then gradients and updates used in backpropagation.  For a softmax classifier, the negative log likelihood can be used as a cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = -T.mean(T.log(softmax.output)[T.arange(y.shape[0]), y])\n",
    "# collect our parameters\n",
    "params = [hidden.params + softmax.params][0]\n",
    "# The gradient for each parameter\n",
    "gparams = [T.grad(cost, p) for p in params]\n",
    "# the general update rule\n",
    "learning_rate = 0.01\n",
    "updates = [(p, p - learning_rate * g) for p, g in zip(params, gparams)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in a test dataset - the iris data is a trivial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "raw = load_iris()\n",
    "train_set_x = theano.shared(np.asarray(raw['data'], dtype=theano.config.floatX), borrow=True)\n",
    "train_set_y = theano.shared(np.asarray(raw['target'], dtype=np.int32), borrow=True)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model, hope it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train_batches = len(raw['data']) / batch_size\n",
    "epochs = 200\n",
    "\n",
    "for e in xrange(epochs):\n",
    "    for batch in xrange(n_train_batches):\n",
    "        train_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:8: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 0 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
      "/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:8: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int32, vector)>.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n"
     ]
    }
   ],
   "source": [
    "predict_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs = softmax.output,\n",
    "        givens={\n",
    "            x: train_set_x[0::,],\n",
    "            y: train_set_y[0::]\n",
    "        },\n",
    "        on_unused_input='warn'\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2,\n",
       "       1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predict_model(0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93999999999999995"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((raw['target'] == np.argmax(predict_model(0), axis=1)).astype(int))/150.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this is accuracy on the training set and not a validation or test set - consider this a proof of concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
